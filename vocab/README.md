**Wikipedia data download** 

The vocabulary was trained using data from the newest available Portuguese Wikipedia dump, from the link https://dumps.wikimedia.org/ptwiki/20200601/ptwiki-20200601-pages-articles.xml.bz2.

**Data preprocessing**

The raw wikidump was preprocessed using [WikiExtractor](https://github.com/attardi/wikiextractor.git) script. Below there is a command line call example with the corresponding expected output (only last lines).

```
Command: ./wikiextractor/WikiExtractor.py ptwiki-20200601-pages-articles.xml -o ptwiki-parsed-full

Final output:
INFO: 6286558   Bairro SAAL da Meia Praia
INFO: 6286559   Capela Nossa Senhora de Montserrat
INFO: 6286560   JoÃ£o Paulo Diniz (jornalista)
INFO: Finished 7-process extraction of 1034123 articles in 967.1s (1069.3 art/s)
INFO: total of page: 1608355, total of articl page: 1034123; total of used articl page: 1034123
```

After the script is run, there will be directories containing processed files with similar size. The ./bash/clean_shuffle_merge.sh script performs the final cleansing (HTML tags removal, empty line deletions and shuffling of all lines) and finally concatenate all files into a single one.

```bash
./clean_shuffle_merge.sh ptwiki-parsed-full wikidump_clean_shuffle_merge.txt
```

**Tokenizer training**

The tokenizer is trained by feeding the cleaned file to [SentencePiece](https://github.com/google/sentencepiece):
```
python3 ./python/train_sentencepiece.py -i ./data/wikidump_clean_shuffle_merge.txt -m spm_32000_pt > ./log_chamada_comando.log 2>&2

```
The files generated by this command are available in the folder `./saved_models/spm_32000_unigram` .

**Using the tokenizer**

We made this tokenizer available for use through direct download on [ðŸ¤—Transformers API](https://github.com/huggingface/transformers), so it's probably easier to use it this way. For that, one can follow the instructions on this repo's main page README (this is the tokenizer used on all models containing `"portuguese-vocab"` on its name).

However, if you wish to use this tokenizer with SentencePiece, or Loading into HuggingFace from local files, the code below can be used as an example:

```python
import sentencepiece as spm         
from transformers import T5Tokenizer

# Path to SentencePiece model
SP_MODEL_PATH = './saved_models/spm_32000_unigram/spm_32000_pt.model'

# Loading on sentencepiece
sp = spm.SentencePieceProcessor()
sp.load(SP_MODEL_PATH)

# Loading on HuggingFace T5Tokenizer
tokenizer = T5Tokenizer(vocab_file=SP_MODEL_PATH)
```
